{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/thotayogeswarreddy/TinyML_ECG_Data/blob/main/TinyECG.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x7_1sRChUM0l"
      },
      "outputs": [],
      "source": [
        "\n",
        "import wfdb\n",
        "import numpy as np\n",
        "import glob\n",
        "import scipy.signal\n",
        "import pandas as pd\n",
        "from scipy.stats import entropy\n",
        "#import tensorflow as tf\n",
        "from scipy.stats import skew, kurtosis\n",
        "import timeit\n",
        "import os\n",
        "from biosppy.signals import ecg\n",
        "from sklearn import svm\n",
        "from sklearn.datasets import make_blobs\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jyxlndixNWhF"
      },
      "outputs": [],
      "source": [
        "pip install biosppy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "mMsa3p-NkZHW",
        "outputId": "9768d829-ee8e-4f26-90fb-62c48c21d24a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nfrom skl2onnx import convert_sklearn\\nfrom skl2onnx.common.data_types import FloatTensorType\\nimport onnxmltools.utils\\nimport onnxmltools\\nimport joblib\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn import svm\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.inspection import permutation_importance\n",
        "#conversion\n",
        "'''\n",
        "from skl2onnx import convert_sklearn\n",
        "from skl2onnx.common.data_types import FloatTensorType\n",
        "import onnxmltools.utils\n",
        "import onnxmltools\n",
        "import joblib\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U4tWlIHUURtq"
      },
      "outputs": [],
      "source": [
        "features=[]\n",
        "labels=[]\n",
        "DatabaseF=[]\n",
        "WDatabase=[]\n",
        "#trained_signals=[]\n",
        "wn=0.1 #cutoff freq\n",
        "beats=0\n",
        "channel_number = 0 # Replace with the appropriate channel number for the signal of interest\n",
        "#Database=['s0014lre','s0020are','s0015lre','s0017lre','s0021are','s0036lre','s0035_re','s0028lre','s0026lre','s0022lre']\n",
        "Database = glob.glob('/content/drive/My Drive/TinyML/s0*')\n",
        "\n",
        "for i in range(len(Database)):\n",
        "  WDatabase.append(Database[i].split('.')[0])\n",
        "\n",
        "DatabaseF = list(set(WDatabase))\n",
        "print(list(DatabaseF))\n",
        "print(len(DatabaseF))\n",
        "\n",
        "ids=[i for i in range(len(DatabaseF))]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HpeGQ-xOUYiM"
      },
      "outputs": [],
      "source": [
        "def bw_filter(signal):\n",
        "    b, a=scipy.signal.butter(4, wn, 'low', analog=False)\n",
        "    filt_ecg=scipy.signal.filtfilt(b,a,signal)\n",
        "    return filt_ecg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sY2Ckv-MUcx0"
      },
      "outputs": [],
      "source": [
        "def extract_features(pqrst_templates, id):\n",
        "    global features, labels\n",
        "    for i, template in enumerate(pqrst_templates):\n",
        "        # Convert the template to a numpy array for easier calculations\n",
        "        template_array = np.array(template)\n",
        "\n",
        "        # Calculate the features\n",
        "        mean_value = np.mean(template_array)\n",
        "\n",
        "        variance_value = np.var(template_array)\n",
        "\n",
        "        std_dev_value = np.std(template_array)\n",
        "\n",
        "        median_value = np.median(template_array)\n",
        "\n",
        "        peak_to_peak_interval = np.ptp(template_array)\n",
        "\n",
        "        skewness_value = skew(template_array)\n",
        "\n",
        "        kurtosis_value = kurtosis(template_array)\n",
        "        zero_crossing_rate = (template_array[:-1] * template_array[1:] < 0).sum()\n",
        "\n",
        "        # Append the features and label to the lists mean_value,peak_to_peak_interval,variance_value,skewness_value,std_dev_value,kurtosis_value,median_value,zero_crossing_rate\n",
        "        features.append([peak_to_peak_interval,skewness_value,std_dev_value,variance_value,mean_value,median_value,kurtosis_value,zero_crossing_rate])\n",
        "        labels.append(id)  # Append the id to the labels list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a712zN0-UhHU"
      },
      "outputs": [],
      "source": [
        "for i in range(len(DatabaseF)):\n",
        "        record_name = DatabaseF[i]\n",
        "        record = wfdb.rdrecord(DatabaseF[i])\n",
        "        signal = record.p_signal[:, channel_number]\n",
        "        sampling_frequency = record.fs\n",
        "        '''\n",
        "        # Create a pandas DataFrame from the ECG signal list\n",
        "        df = pd.DataFrame({\"ECG Signal\": signal})\n",
        "\n",
        "        # Define the file path to save the CSV file\n",
        "        csv_file_path = \"/content/drive/My Drive/TinyML/CSV_Files/\"+str(i)+\".csv\"\n",
        "\n",
        "        # Save the DataFrame to a CSV file\n",
        "        df.to_csv(csv_file_path, index=False)\n",
        "        '''\n",
        "        #filtered_signal= moving_average(signal, window_size)\n",
        "        BWfilteredsignal= bw_filter(signal)\n",
        "        processed_ecg = ecg.ecg(BWfilteredsignal, sampling_rate=sampling_frequency,show=False)\n",
        "        filtered_signal =processed_ecg['filtered']\n",
        "        r_peaks = processed_ecg['rpeaks']\n",
        "        waves = ecg.extract_heartbeats(signal=filtered_signal, rpeaks=r_peaks, sampling_rate=sampling_frequency)\n",
        "        #p_wave, q_wave, r_wave, s_wave, t_wave = waves['templates']\n",
        "        templates_list = waves['templates']\n",
        "        #print(record_name)\n",
        "        beats=beats+len(templates_list)\n",
        "        #print(\"No, of beats \"+ str(len(templates_list)))\n",
        "        #trained_signals.append(filtered_signal)\n",
        "        #extract_features(templates_list, ids[i])\n",
        "        # Create a pandas DataFrame from the ECG signal list\n",
        "        #print(i, len(templates_list))\n",
        "        '''\n",
        "        for j in range(0,len(templates_list)):\n",
        "\n",
        "          df = pd.DataFrame({\"ECG Signal\": templates_list[j]})\n",
        "          folder_name = str(i)\n",
        "          folder_path = os.path.join(\"/content/drive/My Drive/TinyML/test\", folder_name)\n",
        "          os.makedirs(folder_path, exist_ok=True)\n",
        "\n",
        "          # Define the file path to save the CSV file\n",
        "          csv_file_path = os.path.join(folder_path, f\"{i}_B{j}.csv\")\n",
        "\n",
        "          # Save the DataFrame to a CSV file\n",
        "          df.to_csv(csv_file_path, index=False)\n",
        "\n",
        "'''\n",
        "        %matplotlib inline\n",
        "        %config InlineBackend.figure_format = 'retina'\n",
        "        font = {'weight': 'bold',\n",
        "        'size': 14}\n",
        "\n",
        "        plt.figure(figsize=(12, 10))\n",
        "        plt.subplot(3, 1, 1)\n",
        "        plt.plot(signal, label='Original ECG Signal')\n",
        "        plt.title('Original ECG Signal',fontdict=font)\n",
        "        plt.xlabel('Sample Index',fontdict=font)\n",
        "        plt.ylabel('Amplitude', fontdict=font)\n",
        "        plt.legend(prop=font)\n",
        "\n",
        "        plt.subplot(3, 1, 2)\n",
        "        plt.plot(filtered_signal, label='Filtered ECG Signal')\n",
        "        plt.title('Filtered ECG Signal',fontdict=font)\n",
        "        plt.xlabel('Sample Index',fontdict=font)\n",
        "        plt.ylabel('Amplitude',fontdict=font)\n",
        "        plt.legend(prop=font)\n",
        "\n",
        "        plt.subplot(3, 1, 3)\n",
        "        for template in waves['templates']:\n",
        "            plt.plot(template, label='Wave Template')\n",
        "        plt.title('Segmented Waves (P, Q, R, S, T). Total segments or heart beats= ' +str(len(templates_list)),fontdict=font )\n",
        "        plt.xlabel('Sample Index',fontdict=font)\n",
        "        plt.ylabel('Amplitude',fontdict=font)\n",
        "        plt.legend()\n",
        "\n",
        "        plt.subplots_adjust(hspace = 1)\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "#print(beats)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EFXWsYfYQMPZ",
        "outputId": "8f9928a7-222b-4691-902f-314376e07bd7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uOtWJ5V3YaBR"
      },
      "outputs": [],
      "source": [
        "#data preparation\n",
        "features = np.array(features)\n",
        "labels = np.array(labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "222I6zC5kknG"
      },
      "outputs": [],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=0.2, random_state=42)\n",
        "\n",
        "# Normalize the features\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Train the SVM model\n",
        "model = svm.SVC(kernel='linear')  # You can experiment with different kernels ('linear', 'poly', 'rbf', etc.)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Retrieve coefficients\n",
        "coefficients = model\n",
        "\n",
        "# Compute permutation importance\n",
        "perm_importance = permutation_importance(model, X_test, y_test, n_repeats=2, random_state=42)\n",
        "\n",
        "# Extract normalized feature importance\n",
        "normalized_importance = perm_importance.importances_mean / np.linalg.norm(perm_importance.importances_mean)\n",
        "\n",
        "feature_names = ['peak_to_peak_interval','skewness_value','std_dev_value','variance_value','mean_value','median_value','kurtosis_value','zero_crossing_rate']\n",
        "\n",
        "# Sort feature importance and feature names in descending order\n",
        "sorted_indices = np.argsort(normalized_importance)[::-1]\n",
        "sorted_importance = normalized_importance[sorted_indices]\n",
        "sorted_feature_names = np.array(feature_names)[sorted_indices]\n",
        "\n",
        "font = {'weight': 'bold',\n",
        "        'size': 14}\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "bars = plt.barh(np.arange(len(sorted_feature_names)), sorted_importance, color='skyblue')\n",
        "plt.xlabel('Normalized Feature Importance', fontdict=font)\n",
        "plt.ylabel('Feature', fontdict=font)\n",
        "plt.title('Normalized Feature Importance in SVM Model', fontdict=font)\n",
        "plt.yticks(np.arange(len(sorted_feature_names)), sorted_feature_names,fontdict=font)  # Set y-ticks to display feature names\n",
        "plt.gca().invert_yaxis()  # Invert y-axis for better visualization\n",
        "# Annotate each bar with its importance value\n",
        "for bar in bars:\n",
        "    plt.text(bar.get_width(), bar.get_y() + bar.get_height() / 2, f'{bar.get_width() * 100:.2f}%',\n",
        "             va='center', ha='left')\n",
        "%matplotlib inline\n",
        "%config InlineBackend.figure_format = 'retina'\n",
        "file_path = '/content/drive/My Drive/TinyML/plot.png'\n",
        "plt.savefig(file_path)\n",
        "plt.show()\n",
        "'''\n",
        "# Save the trained SVM model to a file using joblib\n",
        "joblib.dump(model, '/content/drive/My Drive/TinyML/svm_model.pkl')\n",
        "\n",
        "# Convert the scikit-learn model to ONNX format\n",
        "initial_type = [('float_input', FloatTensorType([None, len(X_train[0])]))]\n",
        "onnx_model = convert_sklearn(model, initial_types=initial_type)\n",
        "\n",
        "# Save the ONNX model to a file\n",
        "onnxmltools.utils.save_model(onnx_model, '/content/drive/My Drive/TinyML/svm_model.onnx')\n",
        "'''\n",
        "\n",
        "# Test the model\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate the accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Model accuracy: {accuracy}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_yElIpCEJqFd",
        "outputId": "f1e577fa-5f52-4da0-d5cb-32a25bcb10b7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Average Sensitivity: 0.3663663353402864\n",
            "Average Specificity: 0.9980075949388846\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "# Calculate confusion matrix\n",
        "conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Calculate sensitivity and specificity for each class\n",
        "sensitivity_per_class = []\n",
        "specificity_per_class = []\n",
        "for i in range(conf_matrix.shape[0]):\n",
        "    tp = conf_matrix[i, i]\n",
        "    fn = np.sum(conf_matrix[i, :]) - tp\n",
        "    fp = np.sum(conf_matrix[:, i]) - tp\n",
        "    tn = np.sum(conf_matrix) - tp - fn - fp\n",
        "\n",
        "    sensitivity = tp / (tp + fn)\n",
        "    specificity = tn / (tn + fp)\n",
        "\n",
        "    sensitivity_per_class.append(sensitivity)\n",
        "    specificity_per_class.append(specificity)\n",
        "\n",
        "# Calculate average sensitivity and specificity across all classes\n",
        "average_sensitivity = np.mean(sensitivity_per_class)\n",
        "average_specificity = np.mean(specificity_per_class)\n",
        "\n",
        "print(\"Average Sensitivity:\", average_sensitivity)\n",
        "print(\"Average Specificity:\", average_specificity)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LFhn3rF1tbNR",
        "outputId": "1326944b-ebeb-47ee-d53f-6584cda981c1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Current Working Directory: /content\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "# Check the current working directory\n",
        "current_directory = os.getcwd()\n",
        "print(\"Current Working Directory:\", current_directory)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_6Ldsifso_Oh"
      },
      "outputs": [],
      "source": [
        "pip install scikit-learn onnxmltools onnx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uceCqKureF65"
      },
      "outputs": [],
      "source": [
        "X, y = make_blobs(n_samples=290, centers=290, random_state=6)\n",
        "clf = svm.SVC(kernel='linear', C=1000)\n",
        "clf.fit(X, y)  # Train the classifier\n",
        "\n",
        "# Scatter data points\n",
        "plt.scatter(X[:, 0], X[:, 1], c=y, s=100, cmap=plt.cm.tab20)\n",
        "\n",
        "# Plot decision function\n",
        "ax = plt.gca()\n",
        "xlim = ax.get_xlim()\n",
        "ylim = ax.get_ylim()\n",
        "\n",
        "xx = np.linspace(xlim[0], xlim[1], 30)\n",
        "yy = np.linspace(ylim[0], ylim[1], 30)\n",
        "YY, XX = np.meshgrid(yy, xx)\n",
        "xy = np.vstack([XX.ravel(), YY.ravel()]).T\n",
        "#Z = clf.decision_function(xy)\n",
        "#Z = Z.reshape(XX.shape)\n",
        "# Plot decision boundary and margins\n",
        "ax.contour(XX, YY, colors='k', levels=[-1, 0, 1], alpha=0.5, linestyles=['--', '-', '--'])\n",
        "\n",
        "# Plot support vectors\n",
        "ax.scatter(clf.support_vectors_[:, 0], clf.support_vectors_[:, 1], s=100,\n",
        "           facecolors='none', edgecolors='k', marker='o')\n",
        "\n",
        "# Add labels and title\n",
        "plt.title('SVM plot of 290 Subjects')\n",
        "plt.xlabel('Features set (1 of 2 Dimension)')\n",
        "plt.ylabel('Features set (2 of 2 Dimension)')\n",
        "\n",
        "# Modify legend to represent your five ECG signal classes\n",
        "'''\n",
        "class_1 = plt.scatter([], [], s=30, marker='o', color=plt.cm.Paired(0))\n",
        "class_2 = plt.scatter([], [], s=30, marker='o', color=plt.cm.Paired(1))\n",
        "class_3 = plt.scatter([], [], s=30, marker='o', color=plt.cm.Paired(2))\n",
        "class_4 = plt.scatter([], [], s=30, marker='o', color=plt.cm.Paired(3))\n",
        "class_5 = plt.scatter([], [], s=30, marker='o', color=plt.cm.Paired(4))\n",
        "class_6 = plt.scatter([], [], s=30, marker='o', color=plt.cm.Paired(5))\n",
        "plt.legend((class_1, class_2, class_3, class_4, class_5), ('ECG Signal 1', 'ECG Signal 2', 'ECG Signal 3', 'ECG Signal 4', 'ECG Signal 5'), title=\"Classes\")\n",
        "'''\n",
        "%matplotlib inline\n",
        "%config InlineBackend.figure_format = 'retina'\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cL6ygpbWSIaG"
      },
      "outputs": [],
      "source": [
        "import cProfile\n",
        "\n",
        "cProfile.run('extract_features')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YOqyuvfVacHt"
      },
      "outputs": [],
      "source": [
        "features_train, features_test, labels_train, labels_test = train_test_split(features, labels, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kPyHsCZzZQW4"
      },
      "outputs": [],
      "source": [
        "#model creation\n",
        "model = tf.keras.models.Sequential([\n",
        "  tf.keras.layers.Dense(32, activation='relu'),\n",
        "  tf.keras.layers.Dense(16, activation='relu'),\n",
        "  tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SilgDJerbcNR"
      },
      "outputs": [],
      "source": [
        "#model compliance\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hTAkVU8KbjzS"
      },
      "outputs": [],
      "source": [
        "#model training\n",
        "model.fit(features_train, labels_train, epochs=100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RWrtXyPHbpqF"
      },
      "outputs": [],
      "source": [
        "#model accuracy\n",
        "loss, accuracy = model.evaluate(features_test, labels_test)\n",
        "print(f'Accuracy: {accuracy}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0wBFLoPUVhNz",
        "outputId": "6d0461ef-8296-40b3-d044-fb86dc1d2037"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM/HkuQ0WG2IMERC1RhYH0j",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}